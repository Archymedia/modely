import os
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit
from sklearn.metrics import r2_score
from sklearn.model_selection import KFold
from datetime import datetime, timedelta
import seaborn as sns
from scipy import stats

# Set GPU configuration
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Configure TensorFlow to use all GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(f"Physical GPUs: {len(gpus)}, Logical GPUs: {len(logical_gpus)}")
        # Use MirroredStrategy for multi-GPU training
        strategy = tf.distribute.MirroredStrategy()
        print(f"Number of devices: {strategy.num_replicas_in_sync}")
    except RuntimeError as e:
        print(f"GPU configuration error: {e}")
else:
    print("No GPU found. Using CPU.")
    strategy = tf.distribute.get_strategy()

# Track total execution time
start_time = time.time()

# Data loading and preprocessing functions
def load_data(file_path):
    """Load SP100 data from CSV file."""
    print("Loading data...")
    data = pd.read_csv(file_path)
    return data

def load_index_components(data):
    """Extract index component information from the dataset."""
    print("Extracting index components...")
    # Extract the index components from the dataset
    # This is specific to your data structure
    # Assuming the DataFrame has a sheet or columns indicating index components over time
    index_components = data[data['index_component'] == True]
    return index_components

def adjust_prices(data):
    """Adjust OHLC prices for stock splits."""
    print("Adjusting prices for stock splits...")
    
    # Create a copy to avoid modifying the original data
    data = data.copy()
    
    # Check for zero or near-zero values in Close price to avoid division by zero
    data['Close'] = data['Close'].replace(0, np.nan)
    data['CloseAdj'] = data['CloseAdj'].replace(0, np.nan)
    
    # Calculate adjustment factor safely
    adj_factor = data['CloseAdj'] / data['Close']
    
    # Replace infinite values with NaN
    adj_factor = adj_factor.replace([np.inf, -np.inf], np.nan)
    
    # Calculate adjusted prices
    data['OpenAdj'] = data['Open'] * adj_factor
    data['HighAdj'] = data['High'] * adj_factor
    data['LowAdj'] = data['Low'] * adj_factor
    data['VolumeAdj'] = data['Volume'] / adj_factor
    
    # Handle potential infinity or NaN values from division
    for col in ['OpenAdj', 'HighAdj', 'LowAdj', 'VolumeAdj']:
        data[col] = data[col].replace([np.inf, -np.inf], np.nan)
        # Forward fill within each stock group
        data[col] = data.groupby('ID')[col].fillna(method='ffill')
        # Backward fill any remaining NaNs
        data[col] = data.groupby('ID')[col].fillna(method='bfill')
    
    return data

def calculate_returns(data):
    """Calculate daily returns for each stock."""
    print("Calculating daily returns...")
    # Group by stock ID
    grouped = data.groupby('ID')
    
    # Calculate returns for each stock
    returns_dfs = []
    
    for name, group in grouped:
        group = group.sort_values('Date').copy()
        
        # Ensure CloseAdj has no zero or negative values
        group['CloseAdj'] = group['CloseAdj'].replace(0, np.nan)
        group['CloseAdj'] = group['CloseAdj'].where(group['CloseAdj'] > 0, np.nan)
        
        # Calculate daily returns based on adjusted close
        group['Return'] = group['CloseAdj'].pct_change()
        
        # Calculate additional features - 5-day and 20-day returns
        group['Return_5d'] = group['CloseAdj'].pct_change(periods=5)
        group['Return_20d'] = group['CloseAdj'].pct_change(periods=20)
        
        # Calculate volatility features
        group['Volatility_5d'] = group['Return'].rolling(window=5).std()
        group['Volatility_20d'] = group['Return'].rolling(window=20).std()
        
        # Add log returns (handle zero/negative values)
        group['LogReturn'] = np.log(group['CloseAdj']).diff()
        
        # Replace infinite values with NaN
        for col in ['Return', 'Return_5d', 'Return_20d', 'Volatility_5d', 'Volatility_20d', 'LogReturn']:
            if col in group.columns:
                group[col] = group[col].replace([np.inf, -np.inf], np.nan)
        
        returns_dfs.append(group)
    
    # Combine all dataframes
    result = pd.concat(returns_dfs)
    
    # Handle NaN values by forward filling within each stock group
    numeric_cols = result.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        result[col] = result.groupby('ID')[col].fillna(method='ffill')
        result[col] = result.groupby('ID')[col].fillna(method='bfill')
    
    return result

def create_features(data, window_size=20):
    """Create feature set for the MLP model."""
    print(f"Creating features with window size {window_size}...")
    
    # Group by stock ID
    grouped = data.groupby('ID')
    
    feature_dfs = []
    
    for name, group in grouped:
        group = group.sort_values('Date').copy()
        
        # Create lags of returns as features
        for i in range(1, window_size + 1):
            group[f'Return_lag_{i}'] = group['Return'].shift(i)
            
        # Add price momentum features (handle division by zero)
        for days in [5, 10, 20]:
            shifted_close = group['CloseAdj'].shift(days)
            # Avoid division by zero or very small numbers
            shifted_close = shifted_close.where(shifted_close > 1e-10, np.nan)
            group[f'momentum_{days}d'] = (group['CloseAdj'] / shifted_close) - 1
        
        # Add volume features (handle division by zero)
        for days in [5, 20]:
            rolling_vol = group['VolumeAdj'].rolling(window=days).mean()
            rolling_vol = rolling_vol.where(rolling_vol > 1e-10, np.nan)
            group[f'volume_ratio_{days}d'] = group['VolumeAdj'] / rolling_vol
        
        # Add price range features (handle division by zero)
        open_adj_safe = group['OpenAdj'].where(group['OpenAdj'] > 1e-10, np.nan)
        group['daily_range'] = (group['HighAdj'] - group['LowAdj']) / open_adj_safe
        group['daily_range_5d'] = group['daily_range'].rolling(window=5).mean()
        
        # Replace infinite values with NaN
        numeric_cols = group.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            group[col] = group[col].replace([np.inf, -np.inf], np.nan)
        
        feature_dfs.append(group)
    
    # Combine all dataframes
    result = pd.concat(feature_dfs)
    
    # Drop rows with excessive NaN values (more than 50% of columns)
    nan_threshold = len(result.columns) * 0.5
    result = result.dropna(thresh=nan_threshold)
    
    # For remaining rows, forward fill within each stock group
    numeric_cols = result.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        result[col] = result.groupby('ID')[col].fillna(method='ffill')
        result[col] = result.groupby('ID')[col].fillna(method='bfill')
    
    # Final cleanup - drop any remaining rows with NaN values
    result = result.dropna()
    
    return result

def filter_by_index_components(data, index_components):
    """Filter data to include only stocks that were in the index at each point in time."""
    print("Filtering by index components to avoid survivorship bias...")
    
    # If index_components is the same as data (no filtering needed), return as is
    if index_components is data:
        print("No index filtering applied - using all available data")
        return data
    
    # Check if we have the necessary columns for proper filtering
    if 'ID' in data.columns and 'Date' in data.columns:
        try:
            # Merge on both ID and date to keep only stocks that were in the index
            merged_data = pd.merge(
                data,
                index_components[['ID', 'Date']].drop_duplicates(),  # Only keep unique ID-Date combinations
                on=['ID', 'Date'],
                how='inner'
            )
            print(f"Filtered data from {len(data)} to {len(merged_data)} rows")
            return merged_data
        except Exception as e:
            print(f"Error in filtering: {e}")
            print("Returning original data without filtering")
            return data
    else:
        print("Missing required columns for filtering. Using all data.")
        return data

def prepare_train_test_data(data, split_date='2020-12-31'):
    """Split data into training and testing sets based on date."""
    print(f"Splitting data at {split_date}...")
    
    # Convert date column to datetime if it's not already
    if not pd.api.types.is_datetime64_any_dtype(data['Date']):
        data['Date'] = pd.to_datetime(data['Date'])
    
    # Split data
    train_data = data[data['Date'] <= split_date]
    test_data = data[data['Date'] > split_date]
    
    print(f"Training data shape: {train_data.shape}, Testing data shape: {test_data.shape}")
    
    return train_data, test_data

def normalize_data(train_features, test_features):
    """Normalize features using StandardScaler."""
    print("Normalizing data...")
    
    # Create copies to avoid modifying original data
    train_features = train_features.copy()
    test_features = test_features.copy()
    
    # Select only numeric columns for normalization
    numeric_cols = train_features.select_dtypes(include=['float64', 'int64', 'float32', 'int32']).columns
    print(f"Found {len(numeric_cols)} numeric columns for normalization")
    
    # Check for infinite values before normalization
    print("Checking for infinite values in training data...")
    inf_cols = []
    for col in numeric_cols:
        # Convert to numeric first
        train_features[col] = pd.to_numeric(train_features[col], errors='coerce')
        test_features[col] = pd.to_numeric(test_features[col], errors='coerce')
        
        if np.isinf(train_features[col]).any():
            inf_cols.append(col)
            print(f"Found infinite values in column: {col}")
            # Replace infinite values with NaN and then fill with median
            train_features[col] = train_features[col].replace([np.inf, -np.inf], np.nan)
            test_features[col] = test_features[col].replace([np.inf, -np.inf], np.nan)
            
            # Fill NaN values with the median of the training data
            median_val = train_features[col].median()
            if pd.isna(median_val):
                median_val = 0.0  # If all values are NaN, use 0
            train_features[col] = train_features[col].fillna(median_val)
            test_features[col] = test_features[col].fillna(median_val)
    
    if inf_cols:
        print(f"Replaced infinite values in {len(inf_cols)} columns with median values")
    
    # Check for remaining NaN values
    nan_cols = []
    for col in numeric_cols:
        if train_features[col].isna().any():
            nan_cols.append(col)
            print(f"Found NaN values in column: {col}")
            # Fill remaining NaN values with median
            median_val = train_features[col].median()
            if pd.isna(median_val):
                median_val = 0.0  # If all values are NaN, use 0
            train_features[col] = train_features[col].fillna(median_val)
            test_features[col] = test_features[col].fillna(median_val)
    
    if nan_cols:
        print(f"Filled NaN values in {len(nan_cols)} columns with median values")
    
    # Ensure all columns are float64
    for col in numeric_cols:
        train_features[col] = train_features[col].astype(np.float64)
        test_features[col] = test_features[col].astype(np.float64)
    
    # Initialize scaler
    scaler = StandardScaler()
    
    # Fit on training data and transform both training and testing data
    try:
        train_features[numeric_cols] = scaler.fit_transform(train_features[numeric_cols])
        test_features[numeric_cols] = scaler.transform(test_features[numeric_cols])
        
        # Final check for any remaining issues
        if np.any(np.isnan(train_features[numeric_cols].values)) or np.any(np.isinf(train_features[numeric_cols].values)):
            print("Warning: Still found NaN or inf values after normalization")
            # Replace any remaining problematic values with 0
            train_features[numeric_cols] = train_features[numeric_cols].replace([np.inf, -np.inf, np.nan], 0.0)
            test_features[numeric_cols] = test_features[numeric_cols].replace([np.inf, -np.inf, np.nan], 0.0)
            
    except ValueError as e:
        print(f"Error during scaling: {e}")
        print("Data statistics before scaling:")
        print(train_features[numeric_cols].describe())
        print("Data types:")
        print(train_features[numeric_cols].dtypes)
        raise
    
    return train_features, test_features, scaler

def create_sequences(features, target, sequence_length=1):
    """Create input sequences for the model."""
    X, y = [], []
    
    # Group by stock ID
    for stock_id, group in features.groupby('ID'):
        # Sort by date
        group = group.sort_values('Date')
        
        # Get features and target
        feature_values = group[target].values
        
        # Create sequences
        for i in range(len(feature_values) - sequence_length):
            X.append(feature_values[i:i+sequence_length])
            y.append(feature_values[i+sequence_length])
    
    return np.array(X), np.array(y)

def build_mlp_model(input_shape, hidden_layers=3, neurons=64, dropout_rate=0.2, l2_reg=0.001):
    """Build an MLP model with the given architecture."""
    print(f"Building MLP model with {hidden_layers} hidden layers, {neurons} neurons each...")
    
    with strategy.scope():
        model = Sequential()
        
        # Input layer
        model.add(Dense(neurons, activation='relu', input_shape=input_shape,
                        kernel_regularizer=l2(l2_reg)))
        model.add(Dropout(dropout_rate))
        
        # Hidden layers
        for _ in range(hidden_layers - 1):
            model.add(Dense(neurons, activation='relu', kernel_regularizer=l2(l2_reg)))
            model.add(Dropout(dropout_rate))
        
        # Output layer
        model.add(Dense(1))  # Linear activation for regression
        
        # Compile the model
        model.compile(optimizer='adam', loss='mse')
    
    return model

def random_search_cv(X_train, y_train, X_val, y_val, param_dist, n_iter=10, patience=10):
    """Perform random search for hyperparameter tuning."""
    print(f"Performing random search with {n_iter} iterations...")
    
    # Ensure data types are correct
    X_train = X_train.astype(np.float32)
    y_train = y_train.astype(np.float32)
    X_val = X_val.astype(np.float32)
    y_val = y_val.astype(np.float32)
    
    best_val_loss = float('inf')
    best_model = None
    best_params = None
    
    for i in range(n_iter):
        # Sample parameters
        params = {k: np.random.choice(v) for k, v in param_dist.items()}
        
        print(f"Iteration {i+1}/{n_iter}, Parameters: {params}")
        
        # Build model with sampled parameters
        model = build_mlp_model(
            input_shape=(X_train.shape[1],),
            hidden_layers=params['hidden_layers'],
            neurons=params['neurons'],
            dropout_rate=params['dropout_rate'],
            l2_reg=params['l2_reg']
        )
        
        # Train with early stopping
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=patience,
            restore_best_weights=True,
            verbose=1
        )
        
        try:
            history = model.fit(
                X_train, y_train,
                epochs=500,  # Max epochs
                batch_size=params['batch_size'],
                validation_data=(X_val, y_val),
                callbacks=[early_stopping],
                verbose=0
            )
            
            # Check if this model is the best so far
            val_loss = min(history.history['val_loss'])
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_model = model
                best_params = params
                print(f"New best model found! Validation loss: {best_val_loss:.6f}")
                
        except Exception as e:
            print(f"Error training model with parameters {params}: {e}")
            continue
    
    if best_model is None:
        raise ValueError("No models were successfully trained!")
    
    print(f"Best parameters: {best_params}, Best validation loss: {best_val_loss:.6f}")
    return best_model, best_params, best_val_loss

def simulate_trading_strategy(model, data, scaler, feature_cols, target_col='Return',
                             n_stocks=10, profit_target=0.02, stop_loss=0.02):
    """
    Simulate a trading strategy based on model predictions.
    Each day, buy top n_stocks predicted to rise the most and short n_stocks predicted to fall the most.
    Apply profit target and stop loss for each position.
    """
    print("Simulating trading strategy...")
    
    # Group by date
    grouped_by_date = data.groupby('Date')
    
    trading_results = []
    
    for date, group in grouped_by_date:
        # Skip if less than 2*n_stocks available on this day
        if len(group) < 2 * n_stocks:
            continue
        
        # Prepare features for prediction
        X = group[feature_cols].values
        
        # Normalize features if needed
        if scaler:
            X = scaler.transform(X)
        
        # Make predictions
        predictions = model.predict(X)
        
        # Add predictions to the group
        group_with_preds = group.copy()
        group_with_preds['Predicted_Return'] = predictions
        
        # Sort by predictions
        sorted_stocks = group_with_preds.sort_values('Predicted_Return', ascending=False)
        
        # Select top n_stocks for long positions
        long_positions = sorted_stocks.head(n_stocks)
        
        # Select bottom n_stocks for short positions
        short_positions = sorted_stocks.tail(n_stocks)
        
        # Process long positions
        for _, stock in long_positions.iterrows():
            # Get the next day's data for this stock
            next_date = date + pd.Timedelta(days=1)
            next_day_data = data[(data['Date'] == next_date) & (data['ID'] == stock['ID'])]
            
            if not next_day_data.empty:
                next_day = next_day_data.iloc[0]
                
                # Entry price = current day's close
                entry_price = stock['CloseAdj']
                
                # Set profit target and stop loss levels
                pt_level = entry_price * (1 + profit_target)
                sl_level = entry_price * (1 - stop_loss)
                
                # Check if PT or SL was hit
                hit_pt = next_day['HighAdj'] >= pt_level
                hit_sl = next_day['LowAdj'] <= sl_level
                
                # Determine outcome
                if hit_pt and hit_sl:
                    # If both PT and SL were hit, decide based on which was likely hit first
                    # Using a simple heuristic: which threshold was exceeded more
                    pt_excess = (next_day['HighAdj'] - pt_level) / entry_price
                    sl_excess = (sl_level - next_day['LowAdj']) / entry_price
                    
                    if pt_excess > sl_excess:
                        pnl = profit_target
                        outcome = 'PT'
                    else:
                        pnl = -stop_loss
                        outcome = 'SL'
                elif hit_pt:
                    pnl = profit_target
                    outcome = 'PT'
                elif hit_sl:
                    pnl = -stop_loss
                    outcome = 'SL'
                else:
                    # Neither PT nor SL was hit, use close price
                    pnl = (next_day['CloseAdj'] - entry_price) / entry_price
                    outcome = 'CLOSE'
                
                trading_results.append({
                    'Date': date,
                    'ID': stock['ID'],
                    'Position': 'LONG',
                    'Entry': entry_price,
                    'Exit_Price': entry_price * (1 + pnl),
                    'PnL_Pct': pnl,
                    'Outcome': outcome,
                    'Predicted_Return': stock['Predicted_Return']
                })
        
        # Process short positions
        for _, stock in short_positions.iterrows():
            # Get the next day's data for this stock
            next_date = date + pd.Timedelta(days=1)
            next_day_data = data[(data['Date'] == next_date) & (data['ID'] == stock['ID'])]
            
            if not next_day_data.empty:
                next_day = next_day_data.iloc[0]
                
                # Entry price = current day's close
                entry_price = stock['CloseAdj']
                
                # Set profit target and stop loss levels (reversed for short positions)
                pt_level = entry_price * (1 - profit_target)
                sl_level = entry_price * (1 + stop_loss)
                
                # Check if PT or SL was hit
                hit_pt = next_day['LowAdj'] <= pt_level
                hit_sl = next_day['HighAdj'] >= sl_level
                
                # Determine outcome
                if hit_pt and hit_sl:
                    # If both PT and SL were hit, decide based on which was likely hit first
                    pt_excess = (pt_level - next_day['LowAdj']) / entry_price
                    sl_excess = (next_day['HighAdj'] - sl_level) / entry_price
                    
                    if pt_excess > sl_excess:
                        pnl = profit_target
                        outcome = 'PT'
                    else:
                        pnl = -stop_loss
                        outcome = 'SL'
                elif hit_pt:
                    pnl = profit_target
                    outcome = 'PT'
                elif hit_sl:
                    pnl = -stop_loss
                    outcome = 'SL'
                else:
                    # Neither PT nor SL was hit, use close price
                    pnl = (entry_price - next_day['CloseAdj']) / entry_price
                    outcome = 'CLOSE'
                
                trading_results.append({
                    'Date': date,
                    'ID': stock['ID'],
                    'Position': 'SHORT',
                    'Entry': entry_price,
                    'Exit_Price': entry_price * (1 - pnl),  # For short positions, pnl is reversed
                    'PnL_Pct': pnl,
                    'Outcome': outcome,
                    'Predicted_Return': stock['Predicted_Return']
                })
    
    # Convert results to DataFrame
    results_df = pd.DataFrame(trading_results)
    
    return results_df

def calculate_portfolio_metrics(trading_results):
    """Calculate performance metrics for the trading strategy."""
    print("Calculating portfolio metrics...")
    
    # Group results by date to calculate daily portfolio returns
    daily_returns = trading_results.groupby('Date')['PnL_Pct'].mean()
    
    # Calculate cumulative returns
    cumulative_returns = (1 + daily_returns).cumprod() - 1
    
    # Calculate annualized return
    trading_days_per_year = 252
    n_days = len(daily_returns)
    annualized_return = ((1 + cumulative_returns.iloc[-1]) ** (trading_days_per_year / n_days)) - 1
    
    # Calculate annualized volatility
    annualized_vol = daily_returns.std() * np.sqrt(trading_days_per_year)
    
    # Calculate Sharpe ratio (assuming 0% risk-free rate for simplicity)
    sharpe_ratio = annualized_return / annualized_vol if annualized_vol > 0 else 0
    
    # Calculate maximum drawdown
    cumulative_returns_series = (1 + daily_returns).cumprod()
    running_max = cumulative_returns_series.cummax()
    drawdown = (cumulative_returns_series / running_max) - 1
    max_drawdown = drawdown.min()
    
    # Calculate win rate
    win_rate = len(trading_results[trading_results['PnL_Pct'] > 0]) / len(trading_results)
    
    # Calculate profit factor
    gross_profit = trading_results[trading_results['PnL_Pct'] > 0]['PnL_Pct'].sum()
    gross_loss = abs(trading_results[trading_results['PnL_Pct'] < 0]['PnL_Pct'].sum())
    profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')
    
    metrics = {
        'Cumulative Return': cumulative_returns.iloc[-1],
        'Annualized Return': annualized_return,
        'Annualized Volatility': annualized_vol,
        'Sharpe Ratio': sharpe_ratio,
        'Max Drawdown': max_drawdown,
        'Win Rate': win_rate,
        'Profit Factor': profit_factor,
        'Total Trades': len(trading_results)
    }
    
    return metrics, daily_returns, cumulative_returns

def visualize_results(model_history, trading_results, metrics, daily_returns, cumulative_returns, split_date):
    """Create visualizations of model performance and trading results."""
    print("Creating visualizations...")
    
    # Create a figure with multiple subplots
    fig = plt.figure(figsize=(20, 16))
    
    # Plot 1: Model loss during training
    plt.subplot(3, 2, 1)
    plt.plot(model_history.history['loss'], label='Training Loss')
    plt.plot(model_history.history['val_loss'], label='Validation Loss')
    plt.axvline(x=len(model_history.history['loss'])-model_history.history['val_loss'].index(min(model_history.history['val_loss'])), 
                color='r', linestyle='--', label='Early Stopping Point')
    plt.title('Model Loss During Training')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    # Plot 2: Predicted vs Actual Returns (scatter plot)
    plt.subplot(3, 2, 2)
    plt.scatter(trading_results['Predicted_Return'], trading_results['PnL_Pct'], alpha=0.3)
    plt.title('Predicted vs Actual Returns')
    plt.xlabel('Predicted Return')
    plt.ylabel('Actual Return')
    plt.grid(True)
    # Add regression line
    slope, intercept, r_value, p_value, std_err = stats.linregress(
        trading_results['Predicted_Return'], trading_results['PnL_Pct']
    )
    x = np.linspace(trading_results['Predicted_Return'].min(), trading_results['Predicted_Return'].max(), 100)
    plt.plot(x, slope * x + intercept, 'r-', label=f'RÂ² = {r_value**2:.4f}')
    plt.legend()
    
    # Plot 3: Cumulative Returns
    plt.subplot(3, 2, 3)
    plt.plot(cumulative_returns.index, cumulative_returns.values)
    # Add vertical line for train/test split
    split_date_dt = pd.to_datetime(split_date)
    if split_date_dt in cumulative_returns.index:
        plt.axvline(x=split_date_dt, color='r', linestyle='--', label='Train/Test Split')
    plt.title('Cumulative Returns')
    plt.xlabel('Date')
    plt.ylabel('Return')
    plt.legend()
    plt.grid(True)
    
    # Plot 4: Daily Returns Distribution
    plt.subplot(3, 2, 4)
    sns.histplot(daily_returns, kde=True)
    plt.title('Distribution of Daily Returns')
    plt.xlabel('Return')
    plt.ylabel('Frequency')
    plt.grid(True)
    
    # Plot 5: Outcome Distribution
    plt.subplot(3, 2, 5)
    outcome_counts = trading_results['Outcome'].value_counts()
    plt.bar(outcome_counts.index, outcome_counts.values)
    plt.title('Trading Outcomes')
    plt.xlabel('Outcome')
    plt.ylabel('Count')
    plt.grid(True)
    
    # Plot 6: Performance metrics table
    plt.subplot(3, 2, 6)
    plt.axis('off')
    metrics_table = plt.table(
        cellText=[[f"{v:.4f}" if isinstance(v, float) else v for v in metrics.values()]],
        colLabels=list(metrics.keys()),
        loc='center'
    )
    metrics_table.auto_set_font_size(False)
    metrics_table.set_fontsize(10)
    metrics_table.scale(1, 2)
    plt.title('Performance Metrics', pad=20)
    
    plt.tight_layout()
    plt.savefig('mlp_model_results.png', dpi=300)
    plt.close()

def main():
    """Main function to run the entire pipeline."""
    print("Starting MLP model pipeline for SP100 stock prediction...")
    
    # Load data
    data_path = "/Users/lindawaisova/Desktop/DP/data/SP_100/Reuters_SP100_Data.csv"
    data = load_data(data_path)
    
    # Print column names to debug
    print("Available columns in the dataset:")
    print(data.columns.tolist())
    print(f"Data shape: {data.shape}")
    print("First few rows:")
    print(data.head())
    
    # Ensure date column is in datetime format
    data['Date'] = pd.to_datetime(data['Date'])
    
    # Filter data for the specified time range (2005-2023)
    data = data[(data['Date'] >= '2005-01-01') & (data['Date'] <= '2023-12-29')]
    
    # Adjust OHLC prices and volume for stock splits
    data = adjust_prices(data)
    
    # Calculate returns
    data = calculate_returns(data)
    
    # Create features
    data = create_features(data, window_size=20)
    
    # Get index components to handle survivorship bias
    # Check if we have an index membership column (could be 'InIndex', 'in_index', or similar)
    index_col_candidates = ['InIndex', 'in_index', 'index_component', 'in_sp100']
    index_col = None
    
    for col in index_col_candidates:
        if col in data.columns:
            index_col = col
            break
    
    if index_col:
        print(f"Found index membership column: {index_col}")
        index_components = data[data[index_col] == 1]
        # Filter data by index components to avoid survivorship bias
        data = filter_by_index_components(data, index_components)
    else:
        print("No index membership column found. Using all available stocks.")
        print("Note: This may introduce survivorship bias.")
        # If no index column is found, use all data
        index_components = data
    
    # Split data into training and testing sets
    split_date = '2020-12-31'
    train_data, test_data = prepare_train_test_data(data, split_date)
    
    # Define feature columns (excluding non-numeric columns and the target)
    exclude_cols = ['ID', 'Date', 'Return', 'Symbol', 'Name']
    feature_cols = [col for col in train_data.columns if col not in exclude_cols]
    
    # Filter to only numeric columns
    numeric_feature_cols = []
    for col in feature_cols:
        if train_data[col].dtype in ['float64', 'int64', 'float32', 'int32']:
            numeric_feature_cols.append(col)
        else:
            print(f"Excluding non-numeric column: {col} (dtype: {train_data[col].dtype})")
    
    feature_cols = numeric_feature_cols
    print(f"Selected {len(feature_cols)} numeric feature columns")
    
    # Normalize features
    train_data, test_data, scaler = normalize_data(train_data, test_data)
    
    # Prepare data for model training - ensure proper data types
    X_train = train_data[feature_cols].values.astype(np.float32)
    y_train = train_data['Return'].values.astype(np.float32)
    X_test = test_data[feature_cols].values.astype(np.float32)
    y_test = test_data['Return'].values.astype(np.float32)
    
    print(f"X_train shape: {X_train.shape}, dtype: {X_train.dtype}")
    print(f"y_train shape: {y_train.shape}, dtype: {y_train.dtype}")
    print(f"X_test shape: {X_test.shape}, dtype: {X_test.dtype}")
    print(f"y_test shape: {y_test.shape}, dtype: {y_test.dtype}")
    
    # Final validation checks
    print("Performing final data validation...")
    if np.any(np.isnan(X_train)):
        print("Warning: Found NaN values in X_train")
        X_train = np.nan_to_num(X_train, nan=0.0)
    if np.any(np.isinf(X_train)):
        print("Warning: Found inf values in X_train")
        X_train = np.nan_to_num(X_train, posinf=1.0, neginf=-1.0)
    if np.any(np.isnan(y_train)):
        print("Warning: Found NaN values in y_train")
        y_train = np.nan_to_num(y_train, nan=0.0)
    if np.any(np.isinf(y_train)):
        print("Warning: Found inf values in y_train")
        y_train = np.nan_to_num(y_train, posinf=1.0, neginf=-1.0)
    
    print("Data validation complete.")
    
    # Set up k-fold cross-validation
    kf = KFold(n_splits=10, shuffle=True, random_state=42)
    
    # Define parameter distribution for random search
    param_dist = {
        'hidden_layers': [2],
        'neurons': [32],
        'dropout_rate': [0.2],
        'l2_reg': [0.001],
        'batch_size': [64, 128]
    }
    
    # Perform random search with cross-validation
    best_model = None
    best_val_loss = float('inf')
    best_history = None
    
    for train_idx, val_idx in kf.split(X_train):
        # Split data into train and validation
        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]
        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]
        
        # Perform random search
        model, params, val_loss = random_search_cv(
            X_train_fold, y_train_fold,
            X_val_fold, y_val_fold,
            param_dist,
            n_iter=20,  # Reduced iterations for faster execution
            patience=5 # Reduced iterations for faster execution
        )
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model = model
            
            # Train the best model on the entire training set
            early_stopping = EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            )
            
            history = best_model.fit(
                X_train, y_train,
                epochs=10,  # Reduced epochs for faster execution
                batch_size=params['batch_size'],
                validation_split=0.2,
                callbacks=[early_stopping],
                verbose=1
            )
            
            best_history = history
    
    # Evaluate model on test set
    test_loss = best_model.evaluate(X_test, y_test)
    print(f"Test loss: {test_loss}")
    
    # Make predictions on test set
    test_predictions = best_model.predict(X_test)
    
    # Calculate R2 score
    r2 = r2_score(y_test, test_predictions)
    print(f"R2 score: {r2}")
    
    # Simulate trading strategy
    test_data_with_features = test_data.copy()
    test_data_with_features['Predicted_Return'] = test_predictions
    
    trading_results = simulate_trading_strategy(
        best_model, test_data, scaler, feature_cols,
        target_col='Return', n_stocks=10,
        profit_target=0.02, stop_loss=0.02
    )
    
    # Calculate portfolio metrics
    metrics, daily_returns, cumulative_returns = calculate_portfolio_metrics(trading_results)
    
    # Print metrics
    print("\nPortfolio Performance Metrics:")
    for key, value in metrics.items():
        print(f"{key}: {value:.4f}" if isinstance(value, float) else f"{key}: {value}")
    
    # Visualize results
    visualize_results(best_history, trading_results, metrics, daily_returns, cumulative_returns, split_date)
    
    # Calculate total execution time
    end_time = time.time()
    execution_time = end_time - start_time
    print(f"\nTotal execution time: {execution_time:.2f} seconds ({execution_time/60:.2f} minutes)")

if __name__ == "__main__":
    main()
